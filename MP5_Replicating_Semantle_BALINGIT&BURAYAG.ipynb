{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i3m9JjeM5U5"
      },
      "source": [
        "# **Mini-project \\#5**\n",
        "\n",
        "Names: Balingit, Andrei Luis & Burayag, Ethan Axl\n",
        "\n",
        "More information on the assessment is found in our Canvas course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxtmCAZwNoeU"
      },
      "source": [
        "# **Load Pre-trained Embeddings**\n",
        "\n",
        "*While you don't have to separate your code into blocks, it might be easier if you separated loading / downloading your data from the main part of your solution. Consider placing all loading of data into the code block below.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I scoured GitHub to look for Filipino or Tagalog Wordlists for this project and these are what I've found according to number of words:\n",
        "\n",
        "1. [GitHub - AustinZuniga: Filipino Wordlist](https://github.com/AustinZuniga/Filipino-wordlist/tree/master) - Filipino Wordlist with 194327 words\n",
        "\n",
        "2. [GitHub - luisligunas: Pinoy Dictionary Scraper](https://github.com/luisligunas/pinoy-dictionary-scraper.git) - Filipino, Tagalog, Cebuano, Hiligaynon, and Ilocano Wordlist with 98794 Filipino words\n",
        "\n",
        "3. [GitHub - jmalonzo: Tagalog Wordlist](https://github.com/jmalonzo/tl-wordlist) - Tagalog Wordlist with 18319 words\n",
        "\n",
        "4. [GitHub - fofajardo: Tagalog Spellcheck Dictionary](https://github.com/fofajardo/tagalog-spellcheck-dictionary.git) = Filipino/Tagalog Wordlist with 17887 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I initially wanted to use the first wordlist dataset, `AustinZuniga's Filipino Wordlist`, primarily because of its large size. But upon skimming the dataset, it contained a lot of non-words and specific proper names that would not do well in the context of the Semantle game.\n",
        "\n",
        "I checked the second dataset, `luisligunas's Pinoy Dictionary Scraper`, and found out that the Filipino words contained within it are mostly proper nouns and loan words; still unsuitable for the Semantle game.\n",
        "\n",
        "And so, I decided to go with the next dataset with the third highest number of words, `jmalonzo's Tagalog Wordlist`. It has data that is mostly clean and semantically consistent. Its only two issues are that it contains words with **many versions of the same root** and **long compound words**, both of which are inherent characteristics of the Tagalog language (highly morphologically complex) and both of which may prove difficult for the Semantle-style game.\n",
        "\n",
        "Because these two issues are not limited by the rubric, I will leave the dataset as is, not pre-processing nor cleaning it whatsoever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because I am on MacOS, I import the Tagalog Wordlist from Jan Alonzo's repository using the following command:\n",
        "\n",
        ">       `wget -O tl_wordlist.txt https://raw.githubusercontent.com/jmalonzo/tl-wordlist/master/tl.wl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-02-20 20:21:00--  https://raw.githubusercontent.com/jmalonzo/tl-wordlist/master/tl.wl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178256 (174K) [text/plain]\n",
            "Saving to: â€˜tl_wordlist.txtâ€™\n",
            "\n",
            "tl_wordlist.txt     100%[===================>] 174.08K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-02-20 20:21:00 (8.52 MB/s) - â€˜tl_wordlist.txtâ€™ saved [178256/178256]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O tl_wordlist.txt https://raw.githubusercontent.com/jmalonzo/tl-wordlist/master/tl.wl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for the word vectors, I'll use fastText Tagalog Wikipedia vectors. I'll use WGET again to import the file directly:\n",
        "\n",
        ">       `wget -O wiki.tl.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.tl.vec`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-02-20 20:21:00--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.tl.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:289a:d600:13:6e38:acc0:93a1, 2600:9000:289a:b200:13:6e38:acc0:93a1, 2600:9000:289a:b400:13:6e38:acc0:93a1, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:289a:d600:13:6e38:acc0:93a1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 175695851 (168M) [binary/octet-stream]\n",
            "Saving to: â€˜wiki.tl.vecâ€™\n",
            "\n",
            "wiki.tl.vec         100%[===================>] 167.56M  9.98MB/s    in 21s     \n",
            "\n",
            "2026-02-20 20:21:21 (8.05 MB/s) - â€˜wiki.tl.vecâ€™ saved [175695851/175695851]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wiki.tl.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.tl.vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because my laptop, an M4 MacBook Air, is capable enough for this task despite the word vector collection's size, I don't think I'll need to create this Jupyter notebook on Google Colab. But I'll still take precautionary measures in loading the data onto memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8YCZLi-N0uR"
      },
      "source": [
        "# **Your Implementation**\n",
        "\n",
        "*Again, you don't have to have everything in one block. Use the notebook according to your preferences with the goal of fulfilling the assessment in mind.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Tagalog Semantic Word Game\n",
        "\n",
        "In this notebook, I am building a semantic guessing game using Tagalog FastText vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 1. Imports and Configuration\n",
        "\n",
        "First, we set up our paths and configuration. \n",
        "\n",
        "We set a `MAX_WORDS_TO_LOAD` cap to keep memory usage reasonable. \n",
        "\n",
        "We also define a Regex pattern to ensure we only accept \"clean\" Tagalog words; lowercase, no numbers or punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gzip\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "\n",
        "WORDLIST_PATH = Path(\"tl_wordlist.txt\")\n",
        "VEC_PATH = Path(\"wiki.tl.vec\")\n",
        "\n",
        "# arbitrary number\n",
        "MAX_WORDS_TO_LOAD = 80_000\n",
        "\n",
        "# we only want lowercase latin letters and Ã±\n",
        "WORD_RE = re.compile(r\"^[a-zÃ±]+$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 2. Defining the Vocabulary\n",
        "\n",
        "We shall use `jmalonzo's Tagalog Wordlist`. \n",
        "\n",
        "If a word isn't in this list, it doesn't get into the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 17164 valid words from wordlist.\n"
          ]
        }
      ],
      "source": [
        "def load_wordlist(path: Path) -> set:\n",
        "\n",
        "    allowed = set()\n",
        "    \n",
        "    # check if file exists to avoid crashing later\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Missing wordlist: {path}\")\n",
        "\n",
        "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            w = line.strip().lower()\n",
        "            # skip empty lines\n",
        "            if not w:\n",
        "                continue\n",
        "            # only keep words that match our clean regex\n",
        "            if WORD_RE.match(w):\n",
        "                allowed.add(w)\n",
        "                \n",
        "    print(f\"Loaded {len(allowed)} valid words from wordlist.\")\n",
        "    return allowed\n",
        "\n",
        "# execute the load\n",
        "allowed_vocab = load_wordlist(WORDLIST_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 3. Loading the Semantic Vectors\n",
        "Now we read the FastText `.vec` file. \n",
        "\n",
        "We iterate through the vector file, but we only keep the row:\n",
        "1. if the word exists in our `allowed_vocab`.\n",
        "2. if it matches our Regex pattern.\n",
        "3. if we haven't hit our `MAX_WORDS` limit yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading vectors from wiki.tl.vec...\n",
            "Successfully loaded matrix shape: (12714, 300)\n"
          ]
        }
      ],
      "source": [
        "def load_fasttext_vec_text(path: Path, allowed: set, max_words: int) -> Tuple[List[str], np.ndarray]:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Missing vectors file: {path}\")\n",
        "\n",
        "    words: List[str] = []\n",
        "    vectors: List[np.ndarray] = []\n",
        "\n",
        "    # Handle both plain .vec and compressed .vec.gz\n",
        "    opener = gzip.open if path.suffix == \".gz\" else open\n",
        "\n",
        "    print(f\"Reading vectors from {path}...\")\n",
        "    \n",
        "    with opener(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        # The first line of a .vec file is usually \"<vocab_size> <dimensions>\"\n",
        "        header = f.readline().strip().split()\n",
        "        if len(header) != 2:\n",
        "            raise ValueError(\"Unexpected .vec header. Expected: '<vocab_size> <dim>'\")\n",
        "        \n",
        "        _, dim_str = header\n",
        "        dim = int(dim_str)\n",
        "\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            \n",
        "            if len(parts) != dim + 1:\n",
        "                continue\n",
        "\n",
        "            token = parts[0].lower()\n",
        "\n",
        "            # --- FILTERING STEPS ---\n",
        "            if token not in allowed:\n",
        "                continue\n",
        "            if not WORD_RE.match(token):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Convert the numbers from strings to a numpy array (float32 for memory efficiency)\n",
        "                vec = np.array(parts[1:], dtype=np.float32)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            words.append(token)\n",
        "            vectors.append(vec)\n",
        "\n",
        "            # Stop if we reach our memory limit\n",
        "            if len(words) >= max_words:\n",
        "                break\n",
        "\n",
        "    if not words:\n",
        "        raise RuntimeError(\"No vectors loaded. Check your wordlist/path and MAX_WORDS_TO_LOAD.\")\n",
        "\n",
        "    # Stack list of arrays into a single 2D Matrix\n",
        "    mat = np.vstack(vectors).astype(np.float32)\n",
        "    print(f\"Successfully loaded matrix shape: {mat.shape}\")\n",
        "    return words, mat\n",
        "\n",
        "# Execute the load\n",
        "words, mat = load_fasttext_vec_text(VEC_PATH, allowed=allowed_vocab, max_words=MAX_WORDS_TO_LOAD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Normalization through **Cosine Similarity**.\n",
        "\n",
        "We need to normalize all vectors and scale them so their length is 1.0 in order to get the denominator to become 1. \n",
        "\n",
        "This simplifies the calculation to just the Dot Product ($A \\cdot B$), which is much faster for the computer to calculate repeatedly during the game.\n",
        "\n",
        "$$\\text{similarity} = \\frac{A \\cdot B}{||A|| \\times ||B||}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l2_normalize_rows(mat: np.ndarray) -> np.ndarray:\n",
        "\n",
        "    # we need to normalize the rows of the matrix so that the L2 norm of each row is 1\n",
        "    # doing this allows us to use Dot Product as a proxy for Cosine Similarity\n",
        "\n",
        "    # calculate the length of each row\n",
        "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0     # avoid divide by zero before returning\n",
        "    return mat / norms\n",
        "\n",
        "# normalize loaded matrix\n",
        "mat = l2_normalize_rows(mat)\n",
        "\n",
        "# create a lookup dictionary for O(1) access speed\n",
        "word_to_idx: Dict[str, int] = {w: i for i, w in enumerate(words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Starting a Game Round\n",
        "Here we pick a random \"Target Word.\" \n",
        "\n",
        "Because we have the matrix, we can instantly calculate the similarity of *every* word in our vocabulary against the target word. \n",
        "\n",
        "This essentially creates an \"Answer Key\" for the entire game before the player even makes a guess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Word (Hidden): kabiyak\n",
            "\n",
            "--- Game Debug Info ---\n",
            "Closest words to target:\n",
            "Rank 1: umiyak (Score: 0.5832)\n",
            "Rank 10: kasintahang (Score: 0.5509)\n",
            "Rank 100: tiyahin (Score: 0.4524)\n"
          ]
        }
      ],
      "source": [
        "# 1. Pick a random target\n",
        "target_idx = random.randrange(len(words))\n",
        "target_word = words[target_idx]\n",
        "target_vec = mat[target_idx]\n",
        "\n",
        "print(f\"Target Word (Hidden): {target_word}\")\n",
        "\n",
        "# 2. Calculate global similarities\n",
        "sims = mat @ target_vec \n",
        "\n",
        "# 3. Create a ranking order (highest similarity first)\n",
        "order = np.argsort(-sims)       # sort descending \n",
        "\n",
        "# remove the target word itself from the hints list\n",
        "order = order[order != target_idx]\n",
        "\n",
        "def show_rank(rank_1_based: int) -> None:\n",
        "    k = rank_1_based - 1\n",
        "    if k < 0 or k >= len(order):\n",
        "        return\n",
        "    idx = order[k]\n",
        "    print(f\"Rank {rank_1_based}: {words[idx]} (Score: {sims[idx]:.4f})\")\n",
        "\n",
        "# Preview the \"neighbors\" (for debugging/narrative purposes)\n",
        "print(\"\\n--- Game Debug Info ---\")\n",
        "print(\"Closest words to target:\")\n",
        "show_rank(1)\n",
        "show_rank(10)\n",
        "show_rank(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Interactive Game Loop\n",
        " \n",
        "We can run this cell to play the game.\n",
        "\n",
        "The loop will ask for input, check the dictionary, and return the similarity score (0.0 to 1.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game Started! Try to guess the word related to Tagalog context.\n",
            "Type 'exit' to quit.\n",
            "Word: manila | Similarity: 0.1955 | Rank: 9389\n",
            "Word: asawa | Similarity: 0.4157 | Rank: 274\n",
            "Word: kahati | Similarity: 0.1962 | Rank: 9340\n",
            "Word: oo | Similarity: 0.1352 | Rank: 11901\n",
            "Word: hindi | Similarity: 0.2801 | Rank: 3983\n",
            "Word: bobo | Similarity: 0.3769 | Rank: 710\n",
            "Word: tanga | Similarity: 0.3099 | Rank: 2529\n",
            "ðŸŽ‰ CORRECTION! The word is KABIYAK! Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Game Started! Try to guess the word related to Tagalog context.\")\n",
        "print(\"Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        guess = input(\"\\nYour guess: \").strip().lower()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nStopping game.\")\n",
        "        break\n",
        "        \n",
        "    if guess == \"exit\" or not guess:\n",
        "        print(f\"Gave up? The word was: {target_word}\")\n",
        "        break\n",
        "\n",
        "    if guess == target_word:\n",
        "        print(f\"ðŸŽ‰ CORRECT! The word is {guess.upper()}! Score: 1.0\")\n",
        "        break\n",
        "\n",
        "    # lookupindex\n",
        "    idx = word_to_idx.get(guess)\n",
        "    \n",
        "    if idx is None:\n",
        "        print(f\"'{guess}' is not in the vocabulary.\")\n",
        "        continue\n",
        "\n",
        "    # calculate score and rank in relation to chosen word\n",
        "    score = float(mat[idx] @ target_vec)\n",
        "    rank = np.where(order == idx)[0][0] + 1\n",
        "    \n",
        "    print(f\"Word: {guess} | Similarity: {score:.4f} | Rank: {rank}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
